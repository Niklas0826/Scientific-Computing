{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group XX (Name 1, Name 2, Name 3, Name 4)\n",
    "\n",
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is about $QR$ factorization via the Gram-Schmidt process and eigenvalues/eigenvectors. Let's start with initializations as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # basic arrays, vectors, matrices\n",
    "import scipy as sp                # matrix linear algebra \n",
    "\n",
    "import matplotlib                 # plotting\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>.output_png { display: table-cell; text-align: center; vertical-align: middle; }</style>\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Gram-Schmidt Process\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course, we discussed how the Gram-Schmidt process can be used for the QR decomposition of a matrix. In this exercise, we try this idea by applying it for the solution of a linear system. We can start by generating a random square matrix $A$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "A = np.random.rand(N,N)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Complete the function `factorize_QR` below to implement the QR factorization of a square matrix by the Gram-Schmidt process. The function should have the following properties:\n",
    "\n",
    "- For the given input matrix $A$, it should return the matrices $Q$ and $R$.\n",
    "- The diagonal of $R$ should have positive entries.\n",
    "- It should terminate with error if the columns of $A$ are not linearly independent.\n",
    "- Note 1: You may use a standard implementation for the scalar product and norm of a vector.\n",
    "- Note 2: For an orthogonal matrix $Q$ it applies $Q^\\top Q = Q Q^\\top =I$. You may use it to check the correctness of your implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_QR(A):\n",
    "    # TO DO  \n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    Q = np.empty((n, n)) # initialisiere matrix Q\n",
    "    u = np.empty((n, n)) # initialisiere matrix u\n",
    "\n",
    "    u[:, 0] = A[:, 0]\n",
    "    Q[:, 0] = u[:, 0] / np.linalg.norm(u[:, 0])\n",
    "\n",
    "    for i in range(1, n):\n",
    "\n",
    "        u[:, i] = A[:, i]\n",
    "        for j in range(i):\n",
    "            u[:, i] -= (A[:, i] @ Q[:, j]) * Q[:, j] # u vektor \n",
    "\n",
    "        Q[:, i] = u[:, i] / np.linalg.norm(u[:, i]) # q vektor\n",
    "\n",
    "    R = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(i, m):\n",
    "            R[i, j] = A[:, j] @ Q[:, i]\n",
    "\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "Q,R = factorize_QR(A)\n",
    "\n",
    "\n",
    "'''\n",
    "I = np.matmul(np.transpose(Q), Q)\n",
    "print(I) #nicht exakt null \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented the QR factorization, we can use it to solve a linear system $A \\mathbf{x} = \\mathbf{b}$. For the test, we generate a random right hand side vector $\\mathbf{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(N)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we substitute $A = QR$ into the linear system we get \n",
    "\\begin{equation}\n",
    "QR \\mathbf{x} = \\mathbf{b}.\n",
    "\\end{equation}\n",
    "If we multiply both sides with $Q^{-1}$ from the left, we get\n",
    "\\begin{equation}\n",
    "Q^{-1} QR \\mathbf{x} = Q^{-1} \\mathbf{b}.\n",
    "\\end{equation}\n",
    "Since $Q^{-1} Q = I$, we have\n",
    "\\begin{equation}\n",
    "R \\mathbf{x} = Q^{-1} \\mathbf{b},\n",
    "\\end{equation}\n",
    "which can be solved easily by the backward substitution method that you have already implemented in the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Complete the function `solve_with_QR` below, which solves a linear equation system $A\\mathbf{x} = \\mathbf{b}$ using the QR factorization. \n",
    "\n",
    "- For the given QR factorization of $A$, it should return the solution $\\mathbf{x}$.\n",
    "- Note 1: You may use a standard implementation for the matrix-matrix product.\n",
    "- Note 2: It should be clear by now how to invert $Q$ easily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_QR(Q,R,b):\n",
    "    # TO DO\n",
    "    C = np.transpose(Q)\n",
    "    y = np.matmul(C , b)\n",
    "    \n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "    x[n-1] = b[n-1]/R[n-1, n-1]\n",
    "    for i in range(n-1,-1,-1):\n",
    "        Sum = b[i]\n",
    "        for j in range(i+1,n):\n",
    "            Sum = Sum - R[i,j] * x[j]\n",
    "        x[i] = Sum / R[i,i]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we check the implementation by calculating the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = solve_with_QR(Q,R,b)\n",
    "r = b - np.matmul(A,x)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the Gram-Schmidt process is easy to understand and implement, it may suffer from numerical errors for larger matrices. Below we demonstrate such a case, for which the Gram-Schmidt process shows an unstable behaviour. We first generate a random square matrix $A$ with $n=50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50;\n",
    "A = np.random.rand(N,N)\n",
    "Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the QR factorization to $A$, we modify then the diagonal elements of $R$ such that they are decaying exponents of $2$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we modify the main diagonal of R\n",
    "for i in range(N):\n",
    "    R[i,i] = 2**(-i)\n",
    "print(R.diagonal())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reconstruct a modified $A$ matrix from $Q$ and $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Q@R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Apply QR factorization to the modified matrix $A$ by the Gram-Schmidt process. Compare the exact values of the main diagonal $R$ with the values obtained from the Gram-Schmidt process in a plot. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Graph Laplacian and Spectral Clustering\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via adjacency matrices, graphs have a deep connection to linear algebra. In this exercise, to goal is to use eigenvalues and eigenvectors of the so-called *Graph Laplacian* matrix to do some data analysis. Specifically, we will consider clustering, i.e. partitioning a data set into disjoint classes.\n",
    "\n",
    "Let's load a test data set consisting of two-dimensional points to illustrate the problem.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Note:** Please download the file `pointcloud.npz` from the OLAT materials folder, and put it in the same directory as this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"pointcloud.npz\")[\"data\"]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter( X[:,0], X[:,1], 6.0, color=\"crimson\" )\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in clustering is to separate meaningful classes among the data items; in this case, we would like to separate the points forming the \"S\" in the middle from the points forming the surrounding circle. We will use a technique called **spectral clustering** to achieve this.\n",
    "\n",
    "The first step is to create an undirected graph $G = (X,E)$ from the data points as follows. $X$ are the vertices of the graph, whose two coordinates are stored in the two columns of the matrix `X`. There is an edge connecting any two (different) points whose (Euclidean) distance is lower than a given threshold $C > 0$, e.g. for $C=1$:\n",
    "\n",
    "$$\n",
    "\\{ x_i, x_j \\} \\in E \\ \\Leftrightarrow \\|x_i - x_j\\|_2 < 1.\n",
    "$$\n",
    "\n",
    "Undirected graphs can be represented via an adjacency matrix, i.e. a matrix $A \\in \\mathbb{R}^{n\\times n}$, where $n$ is the number of data points. Here, $n = 350$. $A$ is given by\n",
    "\n",
    "$$\n",
    "A_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if $\\{ x_i, x_j \\} \\in E \\ \\Leftrightarrow\\ \\|x_i - x_j\\|_2 < 1$}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Clearly, since $G$ is undirected, $A$ is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task:** Write a function to compute the adjacency matrix $A$ as defined above.\n",
    "\n",
    "Note: the entries of $A$ should be either $1$ or $0$, the diagonal entries $A_{ii}$ should be zero, and the result should be symmetric. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph( X ):\n",
    "    \"\"\"compute the adjacency matrix of the graph over the vertices X as given above\"\"\"\n",
    "    n = 350\n",
    "    A = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Dist = X[i] - X[j]\n",
    "            if np.sqrt(Dist[0]**2 + Dist[1]**2) < 1:\n",
    "                A[i, j] = 1\n",
    "            if Dist.all() == 0:\n",
    "                A[i, j] = 0\n",
    "            \n",
    "    return A\n",
    "\n",
    "\n",
    "A = compute_graph(X)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the resulting graph using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph( axes, X, A, color=None ):\n",
    "    \"\"\"create a plot of the graph with 2D vertices X and adjacency matrix A\"\"\"\n",
    "    \n",
    "    from matplotlib.collections import LineCollection\n",
    "\n",
    "    segs = []\n",
    "    for i, row in enumerate(A):\n",
    "        for j, a in enumerate(row):\n",
    "            if a > 0:\n",
    "                segs.append( [X[i,:], X[j,:]] )\n",
    "\n",
    "    lc = LineCollection(segs, linewidths=1.0, alpha=0.05, color='black')\n",
    "    \n",
    "    if color is None:\n",
    "        color = 'crimson'\n",
    "    \n",
    "    axes.scatter(X[:,0], X[:,1], 5.0, c=color, cmap='autumn', antialiased=True, zorder=5 )\n",
    "        \n",
    "    axes.add_artist(lc)\n",
    "    axes.axis('equal')\n",
    "    axes.grid()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are many edges formed within the \"S\" part and the circle part, but few edges between them.\n",
    "\n",
    "The adjacency matrix $A$ can be inspected graphically using a so-called spy plot. This shows only the non-zero entries of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the matrix is very sparsely populated, i.e. there are few non-zero entries, and these are scattered all over the matrix.\n",
    "\n",
    "The idea behind separating the two parts of the dataset is to now form a so-called graph cut. In essence, this means that we want to separate the data points into two classes $X = X_1 \\mathbin{\\unicode{x228D}} X_2$ and remove all edges between vertices in $X_1$ and $X_2$.\n",
    "\n",
    "While the assignment of each vertex to either $X_1$ or $X_2$ is in principle arbitrary, we can associate with each such cut a cost measure. If we choose $X_1$, then $X_2 = X \\setminus X_1$. \n",
    "\n",
    "Let now $E(X,Y)$ denote the number of edges between vertices in $X$ and vertices in $Y$, then the **normalized cut** for a particular choice of $X_1$ is given as:\n",
    "\n",
    "$$\n",
    "\\mathrm{ncut}_{X_1} = \\frac{|E(X_1,X_2)|}{|E(X_1,X)|} + \\frac{|E(X_1,X_2)|}{|E(X_2,X)|}\n",
    "$$\n",
    "\n",
    "The above formula measures how many edges are cut by the partition, relative to the overall number of edges among vertices of $X$, $X_1$, and $X_2$, respectively. An optimal separation, called a **minimal cut**, minimizes $\\mathrm{ncut}_{X_1}$ over all choices of $X_1$.\n",
    "\n",
    "While one could exhaustively try out all possible choices of $X_1$, there is an elegant connection to linear algebra.\n",
    "\n",
    "By a complex mathematical derivation that is not of too much interest here (but is given [here](https://en.wikipedia.org/wiki/Segmentation-based_object_categorization#Segmentation_using_normalized_cuts)) the minimal normalized cut, i.e. the best partition of $X$ that cuts the fewest edges, can be obtained from the eigenvector corresponding to the _smallest non-zero eigenvalue_ of the graph Laplacian $L$, given by \n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}\n",
    "\\mathrm{deg}(v_i) & \\text{if $i = j$} \\\\\n",
    "-1 & \\text{if $\\{v_i, v_j\\} \\in E$} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{deg}(v_i)$ is the degree of $v_i$, that is the number of edges connected to $v_i$. \n",
    "\n",
    "$L$ can also be computed more compactly as $L = D - A$, where $A$ is the adjacency matrix as above and $D$ is the diagonal degree matrix, $D_{ii} = \\mathrm{deg}(v_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task:** Write a function to compute the graph Laplacian $L$ as given above, and visualize the result using a spy plot.\n",
    "\n",
    "Hint: The number of edges connecting to $v_i$ is the sum of entries in the $i$-th row or column of $A$.\n",
    "\n",
    "Note: the entries of $L$ should be either $0$, $-1$ or a positive integer, and $L$ should be symmetric. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_laplacian( A ):\n",
    "    \"\"\"compute the graph Laplacian matrix of the graph given by adjacency matrix A\"\"\"\n",
    "    # TO DO\n",
    "    n = 350\n",
    "    D = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        D[i, i] = sum(A[i])\n",
    "        \n",
    "    \n",
    "    L = D - A\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "L = graph_laplacian(A)\n",
    "plt.spy(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spy plot, $L$ has the nearly the same pattern (called *sparsity structure*) as $A$; they only differ in the diagonal, where $A$ had zero entries.\n",
    "\n",
    "Let's next compute the smallest non-zero eigenvalue of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task**: Write a function to return the smallest non-zero eigenvalue of $L$, given the eigenvalues and corresponding eigenvectors in two arrays `vals` and `vecs`. \n",
    "\n",
    "Notes: \n",
    "- The input to your function is generated by [`numpy.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html), which compute the eigenvalues and eigenvectors of $L$, in unsorted order.\n",
    "- Set the eigenvalues that are numerically close to zero ($< 10^{-9}$) to a very large number (e.g. $10^9$).\n",
    "- Sort the eigenvalues/eigenvectors from smallest to largest. Use [`numpy.argsort`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) function, which instead of sorting an array directly, returns an array of indices such that the input is in sorted order.\n",
    "- Return the eigenvector corresponding to the now left over smallest eigenvalue (i.e. the column of `vals` with the same index as the smallest eigenvalue).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallest_nonzero_eigenvector(vals, vecs):\n",
    "    \"\"\"return the eigenvector to the smallest non-zero eigenvalue of the input L\"\"\"\n",
    "    # TODO\n",
    "    n = 350\n",
    "    a = vals.copy()\n",
    "    b = vecs.copy()\n",
    "    vals_min = np.zeros(n)\n",
    "    vecs_min = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        if a[i] < 10**(-9):\n",
    "            a[i] = a[i]*10**18\n",
    "\n",
    "    c = np.argsort(a)\n",
    "    d = np.argsort(b)\n",
    "\n",
    "    k=0\n",
    "    \n",
    "    for i in c:\n",
    "        vals_min[k] = a[i]\n",
    "        k = k+1\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    return vals_min, vecs_min        \n",
    "    \n",
    "\n",
    "vals, vecs = np.linalg.eig(L)\n",
    "val_min, vec_min = smallest_nonzero_eigenvector(vals, vecs)\n",
    "\n",
    "print(\"smallest non-zero eigenvalue:\", val_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, the smallest non-zero eigenvalue should be approximately $0.4$.\n",
    "\n",
    "The components of the eigenvector can be interpreted as a function that assigns a real number to every vertex, and `vec_min[i]` is the function value for vertex $v_i$. Let's visualize this function; thankfully, `plot_graph` already supports this through an optional `color` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A, color=np.real(vec_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission accomplished! This process is called **spectral clustering**. (\"Spectral\" indicates that this technique is related to eigenvalue analysis.) High function values ($> 0$, yellowish) correspond to the set $X_1$ â€“ the \"S\" -- while low function values ($\\leq 0$, reddish) correspond to the other set $X_2$, the surrounding circle.\n",
    "\n",
    "If everything is correct, the image should look roughly like this (possibly with reversed colors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plot_graph(plt.gca(), X, A, color=(np.linalg.norm(X, axis=1) < 1.5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course this trivial separation of points by radius would not work in the general case, but it works well for this test case.)\n",
    "\n",
    "One more thing: we can reorder the vertices of the graph such that the vertices in $X_1$ are before those of $X_2$. This does interesting things to the adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(vec_min > 0)\n",
    "plt.spy( A[idx].T[idx] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this reorders the adjacency matrix into two large blocks, with a minimal number of entries outside these blocks. The latter are exactly the edges that are cut by the minimal cut above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
